# 119 구급대원 현장응급처치 RAG 챗봇 프로젝트 완벽 가이드

## 📚 목차
1. [프로젝트가 뭔가요?](#프로젝트가-뭔가요)
2. [RAG가 뭔가요?](#rag가-뭔가요)
3. [이 프로젝트는 무엇을 하나요?](#이-프로젝트는-무엇을-하나요)
4. [왜 로컬에서 실행하나요?](#왜-로컬에서-실행하나요)
5. [프로젝트 구조 설명](#프로젝트-구조-설명)
6. [작동 원리 (단계별 설명)](#작동-원리-단계별-설명)
7. [설치 및 실행 방법](#설치-및-실행-방법)
8. [각 파일의 역할](#각-파일의-역할)
9. [기술 용어 설명](#기술-용어-설명)
10. [문제 해결 가이드](#문제-해결-가이드)

---

## 프로젝트가 뭔가요?

이 프로젝트는 **"119 구급대원 현장응급처치 표준지침(2023)"**이라는 PDF 문서를 읽고, 사용자가 질문하면 그 문서를 기반으로 답변을 해주는 **챗봇**입니다.

### 예시
- **질문**: "외상 환자 지혈은 어떻게 하나요?"
- **답변**: 문서에서 해당 내용을 찾아서 설명해주고, 몇 페이지에 나와있는지도 알려줍니다.

---

## RAG가 뭔가요?

**RAG (Retrieval-Augmented Generation)**는 두 가지 단어의 조합입니다:

1. **Retrieval (검색)**: 필요한 정보를 찾아오는 것
2. **Augmented Generation (보강된 생성)**: 찾아온 정보를 바탕으로 답변을 만드는 것

### 일반 챗봇 vs RAG 챗봇

#### 일반 챗봇 (예: ChatGPT)
- 학습한 모든 지식을 기억하고 답변
- 최신 정보나 특정 문서 내용을 정확히 반영하기 어려움

#### RAG 챗봇 (이 프로젝트)
- 질문을 받으면 → 관련 문서에서 정보를 검색 → 검색한 정보를 바탕으로 답변
- 특정 문서(여기서는 119 응급처치 가이드)만을 근거로 정확한 답변 제공
- 답변에 "몇 페이지"에 나와있는지도 알려줌 (출처 명시)

### 왜 RAG를 사용하나요?
- ✅ 정확한 출처 제공 (책의 몇 페이지인지)
- ✅ 특정 문서만을 근거로 답변 (오해의 소지 감소)
- ✅ 문서가 업데이트되면 새로 인덱싱만 하면 됨

---

## 이 프로젝트는 무엇을 하나요?

이 프로젝트는 **119 구급대원**들이 현장에서 응급처치 방법을 빠르게 찾아볼 수 있도록 도와주는 도구입니다.

### 주요 기능
1. **PDF 문서 읽기**: 119 응급처치 표준지침 PDF를 분석
2. **질문 받기**: 사용자가 질문을 입력
3. **관련 내용 찾기**: PDF에서 질문과 관련된 부분 검색
4. **답변 생성**: 찾은 내용을 바탕으로 답변 작성
5. **출처 표시**: 몇 페이지에 나와있는지 표시

---

## 왜 로컬에서 실행하나요?

이 프로젝트는 **모든 처리를 컴퓨터 내부에서** 합니다.

### 로컬 실행의 장점
- ✅ **프라이버시**: 질문 내용이 외부 서버로 전송되지 않음
- ✅ **오프라인 동작**: 인터넷이 없어도 작동 (초기 모델 다운로드 후)
- ✅ **비용**: 무료 (외부 API 비용 없음)
- ✅ **속도**: 인터넷 연결 속도에 영향받지 않음

### 필요한 것들
1. **임베딩 모델**: 텍스트를 숫자로 변환하는 도구 (로컬에 다운로드)
2. **FAISS**: 검색을 빠르게 해주는 도구 (로컬에 설치)
3. **Ollama**: AI 모델을 실행하는 도구 (로컬에 설치, localhost에서 실행)

**중요**: 
- Ollama는 로컬 컴퓨터에서 실행되는 서버입니다 (기본 주소: http://localhost:11434)
- `requests` 라이브러리는 이 로컬 서버와 HTTP 통신하기 위해 사용됩니다
- 인터넷 연결이 필요하지 않습니다 (초기 모델 다운로드 제외)

---

## 프로젝트 구조 설명

```
119_LLM/
├── app.py                    # 메인 프로그램 (사용자가 실행하는 파일)
├── config.py                 # 설정 파일 (모델 이름, 기본값 등)
├── requirements.txt          # 필요한 라이브러리 목록
│
├── data/                     # 원본 데이터
│   └── guide_2023.pdf       # 119 응급처치 가이드 PDF 파일
│
├── ingest/                   # 문서 처리 관련
│   └── build_index.py       # PDF를 읽어서 검색 가능한 형태로 변환
│
├── rag/                      # RAG 시스템 관련 코드
│   ├── retriever.py         # 문서에서 관련 내용 찾기
│   ├── ollama_client.py     # AI 모델과 대화하기
│   ├── prompting.py         # AI에게 질문하는 방식 만들기
│   └── formatting.py        # 결과를 보기 좋게 정리
│
└── artifacts/                # 생성된 인덱스 파일 저장 (빈 폴더, 실행 후 생성됨)
    ├── faiss.index          # 검색용 인덱스 파일
    ├── meta.json            # 청크 메타데이터
    └── chunks.jsonl         # 텍스트 청크 데이터
```

---

## 작동 원리 (단계별 설명)

### 1단계: PDF 인덱싱 (최초 1회만 실행)

```
PDF 파일 → 텍스트 추출 → 작은 조각(청크)으로 나누기 → 숫자로 변환(임베딩) → 검색용 데이터베이스 저장
```

**왜 이렇게 하나요?**
- PDF는 사람이 읽기 좋은 형태이지, 컴퓨터가 검색하기 좋은 형태가 아닙니다
- 컴퓨터는 텍스트를 "숫자"로 변환해야 빠르게 검색할 수 있습니다
- 전체를 한 번에 검색하기보다, 작은 조각으로 나눠서 검색하는 게 정확합니다

### 2단계: 질문 받기

사용자가 질문을 입력합니다.
예: "외상 환자 지혈은 어떻게 하나요?"

### 3단계: 질문을 숫자로 변환

질문 텍스트를 컴퓨터가 이해할 수 있는 숫자(벡터)로 변환합니다.

### 4단계: 유사한 내용 검색

변환된 숫자를 이용해 PDF에서 관련된 조각들을 찾습니다.
- 유사도 점수가 높은 순서로 정렬
- 상위 5개 정도를 선택 (Top-k)

### 5단계: 유사도 체크

찾은 내용이 질문과 얼마나 관련이 있는지 확인합니다.
- **유사도가 낮으면** (기준값 미만): "더 구체적으로 물어봐주세요" 라고 요청
- **유사도가 높으면** (기준값 이상): 다음 단계로 진행

### 6단계: 답변 생성

찾은 내용들을 AI 모델(Ollama)에게 보내서 답변을 만들어달라고 요청합니다.

### 7단계: 결과 표시

생성된 답변과 함께, 어떤 페이지에서 찾았는지 표시합니다.

---

## 설치 및 실행 방법

### 사전 준비
1. Python 3.8 이상 설치
2. Ollama 설치 및 실행
3. Python 가상환경 생성 (권장)

### 단계별 설치

#### 1단계: 가상환경 만들기 (선택사항이지만 권장)

```bash
# 가상환경 생성
python -m venv .venv

# 가상환경 활성화
# macOS/Linux:
source .venv/bin/activate
# Windows:
# .venv\Scripts\activate
```

**가상환경이 뭔가요?**
- 프로젝트별로 필요한 라이브러리를 따로 관리하는 공간
- 다른 프로젝트와 충돌을 방지합니다

#### 2단계: 필요한 라이브러리 설치

```bash
pip install -r requirements.txt
```

이 명령어는 `requirements.txt` 파일에 적힌 모든 라이브러리를 설치합니다:
- `streamlit`: 웹 인터페이스
- `pymupdf`: PDF 읽기
- `sentence-transformers`: 텍스트를 숫자로 변환
- `faiss-cpu`: 빠른 검색
- `numpy`: 숫자 계산
- `requests`: HTTP 통신 (로컬 Ollama 서버와 통신)
- `tqdm`: 진행상황 표시

#### 3단계: Ollama 설치 및 모델 다운로드

1. [Ollama 공식 사이트](https://ollama.ai)에서 Ollama 설치
2. Ollama 실행 (백그라운드에서 돌아가야 함)
3. 모델 다운로드:

```bash
ollama pull llama3.1:8b-instruct
```

**Ollama가 뭔가요?**
- 로컬에서 AI 모델을 실행할 수 있게 해주는 도구
- 인터넷 없이도 AI를 사용할 수 있습니다

**모델이 뭔가요?**
- AI가 학습한 "지식"이 담긴 파일
- 여기서는 텍스트를 생성하는 데 사용됩니다

#### 4단계: PDF 인덱싱 (최초 1회)

```bash
python -m ingest.build_index --pdf data/guide_2023.pdf --out artifacts
```

이 명령어를 실행하면:
- `data/guide_2023.pdf` 파일을 읽습니다
- 텍스트를 추출하고 작은 조각으로 나눕니다
- 각 조각을 숫자로 변환합니다
- `artifacts/` 폴더에 검색용 파일들을 저장합니다

**왜 최초 1회만 하나요?**
- PDF가 바뀌지 않는 한, 인덱스는 계속 사용할 수 있습니다
- PDF가 업데이트되면 다시 실행하면 됩니다

#### 5단계: 프로그램 실행

```bash
streamlit run app.py
```

브라우저가 자동으로 열리며 챗봇 인터페이스가 표시됩니다.

---

## 각 파일의 역할

### 📄 app.py
**역할**: 메인 프로그램, 사용자 인터페이스

**하는 일**:
- Streamlit으로 웹 페이지 만들기
- 사용자 질문 받기
- 다른 모듈들을 연결해서 답변 생성
- 결과를 화면에 표시

### 📄 config.py
**역할**: 프로젝트 설정 저장

**저장하는 것들**:
- 사용할 AI 모델 이름
- 검색할 때 가져올 문서 조각 개수
- 유사도 기준값
- Ollama 서버 주소

### 📄 ingest/build_index.py
**역할**: PDF를 검색 가능하게 변환

**처리 과정**:
1. PDF 파일 열기
2. 각 페이지의 텍스트 추출
3. 불필요한 부분 제거 (헤더, 푸터 등)
4. 텍스트를 작은 조각(청크)으로 나누기
5. 각 청크를 숫자(임베딩)로 변환
6. FAISS 인덱스로 저장

### 📄 rag/retriever.py
**역할**: 질문과 관련된 문서 조각 찾기

**작동 방식**:
1. 질문 텍스트를 숫자(임베딩)로 변환
2. FAISS 인덱스에서 유사한 조각 검색
3. 유사도 점수 높은 순서로 정렬
4. 상위 N개 반환

### 📄 rag/ollama_client.py
**역할**: Ollama AI 모델과 대화하기

**하는 일**:
- Ollama 서버에 HTTP 요청 보내기
- 프롬프트(질문 + 문서 내용) 전달
- AI가 생성한 답변 받아오기

### 📄 rag/prompting.py
**역할**: AI에게 보낼 질문 형식 만들기

**왜 필요한가요?**
- AI는 특정 형식으로 질문받으면 더 정확하게 답변합니다
- 여기서는 "문서 내용만 사용해라", "출처를 명시해라" 같은 규칙을 추가합니다

### 📄 rag/formatting.py
**역할**: 검색 결과를 보기 좋게 정리

**하는 일**:
- 찾은 문서 조각들을 마크다운 형식으로 변환
- 페이지 번호, 유사도 점수 등을 표시

---

## 기술 용어 설명

### 임베딩 (Embedding)
**정의**: 텍스트를 컴퓨터가 이해할 수 있는 숫자(벡터)로 변환하는 것

**비유**: 
- 사람은 "고양이"라는 단어를 보고 고양이를 상상할 수 있습니다
- 컴퓨터는 "고양이"를 보고 이해할 수 없으니, [0.1, 0.5, -0.3, ...] 같은 숫자 배열로 바꿔줍니다
- 비슷한 의미의 단어는 비슷한 숫자 배열을 가집니다

**이 프로젝트에서**: 질문과 문서 조각 모두를 숫자로 변환해서, 비슷한 의미인지 비교합니다

### FAISS
**정의**: Facebook에서 만든 빠른 검색 라이브러리

**왜 사용하나요?**
- 수천, 수만 개의 문서 조각 중에서 빠르게 검색해야 합니다
- 일반적인 방법으로는 느리지만, FAISS를 사용하면 매우 빠릅니다

**비유**: 
- 일반 검색 = 책을 처음부터 끝까지 읽으면서 찾기
- FAISS 검색 = 목차나 색인을 이용해서 바로 해당 페이지 찾기

### 청크 (Chunk)
**정의**: 문서를 작은 조각으로 나눈 것

**왜 나누나요?**
- 전체 문서를 한 번에 검색하면 정확도가 떨어집니다
- 작은 조각으로 나누면, 질문과 정확히 관련된 부분만 찾을 수 있습니다

**비유**:
- 책의 한 쪽 페이지 = 하나의 청크
- 질문이 "3장"과 관련되어 있으면, 3장의 페이지들만 가져옵니다

### 유사도 (Similarity)
**정의**: 두 텍스트가 얼마나 비슷한지 나타내는 점수

**점수 범위**:
- 0.0 ~ 1.0 사이 (이 프로젝트에서는 코사인 유사도 사용)
- 1.0에 가까울수록 매우 비슷함
- 0.0에 가까울수록 전혀 다름

**이 프로젝트에서**:
- 유사도가 0.30 미만이면 "관련 정보를 찾기 어렵습니다"라고 말합니다
- 0.30 이상이면 찾은 내용을 바탕으로 답변을 생성합니다

### 코사인 유사도 (Cosine Similarity)
**정의**: 두 벡터(숫자 배열) 사이의 각도를 측정해서 유사도를 계산

**비유**:
- 두 화살표가 같은 방향을 가리키면 → 유사도 높음 (1.0)
- 반대 방향을 가리키면 → 유사도 낮음 (0.0)

### Top-k
**정의**: 검색 결과 중 상위 k개를 선택하는 것

**이 프로젝트에서**:
- 기본값은 5개 (Top-5)
- 질문과 가장 유사한 5개의 문서 조각을 가져옵니다

### 프롬프트 (Prompt)
**정의**: AI에게 보내는 지시사항이나 질문

**이 프로젝트에서**:
- 사용자 질문 + 찾은 문서 조각들을 합쳐서 프롬프트를 만듭니다
- "이 문서 내용만 사용해서 답변해라" 같은 규칙도 포함합니다

### Streamlit
**정의**: Python으로 웹 인터페이스를 쉽게 만드는 라이브러리

**장점**:
- 복잡한 HTML/CSS/JavaScript 없이도 웹 페이지를 만들 수 있습니다
- 몇 줄의 코드로 챗봇 인터페이스를 만들 수 있습니다

### Ollama
**정의**: 로컬에서 AI 모델을 실행할 수 있게 해주는 도구

**장점**:
- 인터넷 연결 없이도 AI 사용 가능
- 데이터가 외부로 전송되지 않음 (프라이버시)
- 무료

---

## 문제 해결 가이드

### 문제 1: "artifacts/faiss.index 가 없습니다" 오류

**원인**: PDF 인덱싱을 아직 안 했습니다

**해결**:
```bash
python -m ingest.build_index --pdf data/guide_2023.pdf --out artifacts
```

### 문제 2: "Ollama 호출에 실패했습니다" 오류

**원인 1**: Ollama가 실행되지 않음

**해결**:
- Ollama를 실행하세요 (macOS: Spotlight에서 "Ollama" 검색, Windows: 시작 메뉴에서 실행)

**원인 2**: 모델이 다운로드되지 않음

**해결**:
```bash
ollama pull llama3.1:8b-instruct
```

**확인 방법**:
```bash
ollama list
```
다운로드된 모델 목록이 표시됩니다.

### 문제 3: 임베딩 모델 다운로드가 느림

**원인**: 처음 실행 시 Hugging Face에서 모델을 다운로드합니다 (약 420MB)

**해결**: 
- 인터넷 연결이 좋은 곳에서 처음 실행하세요
- 이후에는 캐시를 사용하므로 빠릅니다

### 문제 4: 메모리 부족 오류

**원인**: PDF 파일이 너무 크거나, 컴퓨터 메모리가 부족

**해결**:
- 더 가벼운 Ollama 모델 사용 (예: `qwen2.5:7b-instruct`)
- `config.py`에서 `top_k` 값을 줄이기 (예: 3으로 변경)

### 문제 5: 검색 결과가 부정확함

**해결 방법**:
1. **유사도 임계값 조정**: Streamlit 사이드바에서 `min_sim` 값을 낮춤 (0.20 정도)
2. **Top-k 증가**: 더 많은 문서 조각을 검색 (5 → 7)
3. **질문 구체화**: "지혈" 보다는 "외상 환자 지혈 방법"처럼 구체적으로 질문

---

## 마무리

이 프로젝트는 RAG 기술을 이용해서 특정 문서(PDF)를 기반으로 답변하는 챗봇입니다. 

### 핵심 개념 요약
1. **PDF → 인덱싱**: 문서를 검색 가능하게 변환
2. **질문 → 임베딩**: 질문을 숫자로 변환
3. **검색**: 유사한 문서 조각 찾기
4. **생성**: AI가 답변 생성
5. **표시**: 답변 + 출처 표시

### 다음 단계
- 코드를 읽으면서 각 부분이 어떻게 작동하는지 이해하기
- 설정값을 조정해서 성능 개선하기
- 다른 PDF 파일로 테스트해보기

**주의사항**: 
이 도구는 학습/참고용입니다. 실제 현장 처치는 소속 지침, 의료지도, 법·규정 및 교육을 우선합니다.

---

**작성일**: 2024
**프로젝트**: 119 구급대원 현장응급처치 RAG 챗봇
**목적**: 자연어 처리 분야 초보자를 위한 완벽한 가이드

